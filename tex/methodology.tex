\chapter{Methodology} \label{chap:methodology}
Many computer vision applications such as 3D object reconstruction or visual SLAM require the depth information from RGBD cameras. 
However, the results of these applications are often not unsatisfying because of the low quality of the depth acquisition from the cheap cameras. 
It would be gratifying if we can improve the depth quality without changing to an expensive camera.
Therefore, the depth refinement techniques play an essential role here.

In this chapter, we first introduce some pre-processing techniques to fill the missing areas and reduce the noise of the input depth image. 
Then, we describe in detail one of the state-of-the-art depth refinement method from Or-El \emph{et al.}~\cite{or2015rgbd} which we have chosen to implement as a starting point.
A proposed method based on a RGB ratio model is then followed and introduced to eliminate the nonlinearity in most of modern depth enhancement method.
Finally, another proposed technique which does not require any regularization terms is presented. 
This method has also exhibited the ability of dealing with the objects with complicated albedos and extension to depth super-resolution.
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pre-Processing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The first step for most of the image processing tasks is to pre-process the initial input image. 
Due to the hardware limitation of modern inexpensive RGBD sensors, there usually exist holes with missing values on the depth images. 
Also, the depth data is often noisy so we need to do denoising and acquire a relative smooth surface.

In this section, we will describe respectively the basic depth inpainting and denoising algorithm that we use for our pre-processing. 
%----------------------------------------------
\subsection{Depth inpainting}
%----------------------------------------------

%\begin{figure}[!htbp]
% \centering
% \includegraphics[width=0.45\textwidth]{figures/methodology/inpainting_rgb.png}\includegraphics[width=0.45\textwidth]{figures/methodology/inpainting_depth.pdf}
% \caption{dd}
% \label{fig:inpainting1}
%\end{figure}

Image inpainting itself is a very mutual area and has been widely applied as a useful tool for many modern computer vision applications, e.g, restore the damaged parts of ancient paintings, or remove unwanted texts or objects in a photography~\cite{bertalmio2000image}. 
Since the idea of image inpainting is to automatically replace the lost or undesired parts of an image with the neighbouring information by interpolating, we were inspired to apply it to fill in the missing depth information (Fig.~\ref{fig:inpainting1}).

\begin{figure}[!htbp]
\centering
\subfigure[RGB image]{\includegraphics[width=0.45\linewidth]{figures/methodology/inpainting_rgb.png}}
\subfigure[Input depth image]{\includegraphics[width=0.45\linewidth]{figures/methodology/inpainting_depth.pdf}}
\caption{The input RGB and depth image of a vase. The brighter color on the image (b), the higher depth values.}
\label{fig:inpainting1}
\end{figure}

It should be noted that, the depth inpainting is applied to the input noisy image so there is no need to use some powerful and advanced algorithms.
The only request is to fill the missing areas with inexpensive computational time.

The general mathematical form of a classic inpainting algorithm~\cite{bertalmio2000image} can be written as follows

\begin{equation}\label{eq:method_inpaint1}
I^{t+1}(i,j) = I^{t}(i,j) + \mu U^{t}(i,j), \forall(i,j)\in \Omega
\end{equation}
where $I(i,j)$ is the pixel value in image $I$, $t$ is the artificial time step, $\mu$ is the updating rate, $U$ is the update information and $\Omega$ are the area with missing information.

To build the update map $U$ in each time step, there are two principles that~\cite{bertalmio2000image} follow.
One is the inpainted values inside $\Omega$ should be as smooth as possible. 
The other is the lines reaching the edge of $\Omega$ should be continued and cross the missing area, while the values in $\Omega$ should be propagated from the nearest neighbours of $\Omega$ along the lines.

Again, due to the fact that our input depth images have poor quality, the lines arriving at the boundary $\delta \Omega$ may be incorrect or produced by the noises.
Thus, it is reasonable that our initial depth inpainting problem focuses on the smooth propagation from the neighbours and fill in the holes.

In each pixel $(x_0, y_0)$ inside $\Omega$, $U$ can be modelled as a discrete four-neighbour Laplacian operator:
\begin{equation}
\begin{split}
U(x_0, y_0) = \Delta I
		        = 4I(x_0, y_0) - I(x_0 + 1, y_0) - I(x_0 - 1, y_0) - I(x_0, y_0+1) - I(x_0 , y_0 -1)
\end{split}
\end{equation}
Now the inpainting problem in Eq.~\ref{eq:method_inpaint1} can be represented as a minimization problem: 
\begin{equation}
\min \iint\limits_{\Omega} |U(x,y)|^2 dxdy 
\end{equation}
This problem can be reformulated to a typical linear equation in matrix form:
\begin{equation}
\mathbf{Ax=b}
\end{equation}
Assuming $n$ is the number of pixel inside $\Omega$ and $m$ is the sum of $n$ and the number of neighbouring pixel around the boundary $\delta \Omega$, $\mathbf{A}$ is a $m\times n$ Laplacian matrix, $\mathbf{b}$ is a $m\times1$ vector containing all the known boundary depth values and the $0$ inside $\Omega$.
Solving the linear equation with simple least square method, we can acquire the inpainted values.
With our this naive image inpainting algorithm, we can fill the holes on the depth image as shown in Fig.~\ref{fig:pre-processing}.

%----------------------------------------------
\subsection{Depth denoising}
%----------------------------------------------
the depth images acquired from the RGB-D cameras with moderate price usually contain various noises. 
As a standard pre-processing method, the image denoising technique is also applied to our input inpainted depth map.
Similar to the state-of-the-art depth refinement methods~\cite{zhang2012edge, or2015rgbd, han2013high, or2016real, haque2014high, yu2013shading}, bilateral fitering~\cite{tomasi1998bilateral} is used as our depth pre-processing smoother. 

The advantages of bilateral filter is reducing the noise while preserving the edge in the input image. 
More than a regular Gaussian smooth filter, which uses only the difference of the image values (depth in our case) between the center pixel the neighbours, the bilateral filter also utilizes the space difference as a reference to build up the weighting function.
The filtered pixel value can be modelled as a weighted sum of neighbouring pixels:
\begin{equation}
\hat{I}(\mathbf{x}) = \frac{1}{W}\sum_{\mathbf{y} \in \mathcal{N}} I(\mathbf{y})e^{-(\frac{\lVert I(\mathbf{x}) - I(\mathbf{y})\rVert^2}{2\sigma_r^2} + \frac{\lVert \mathbf{x} - \mathbf{y}\rVert^2}{2\sigma_d^2})}
\end{equation}
where $\hat{I}(\mathbf{x})$ is the filtered value at pixel $\mathbf{x}$, $\mathcal{N}$ represents the neighbouring pixels with $\mathbf{x}$ in the center, and $W$ is the the sum of the all the weights. 
The smoothed result on our input depth image is shown in Fig.~\ref{fig:pre-processing}.

\begin{figure}[!htbp]
\centering
\subfigure[Input depth]{\includegraphics[width=0.30\linewidth]{figures/methodology/inpainting_shape_before.pdf}}
\subfigure[After inpainting]{\includegraphics[width=0.30\linewidth]{figures/methodology/inpainting_shape_after.pdf}}
\subfigure[After smoothing]{\includegraphics[width=0.30\linewidth]{figures/methodology/smooth_shape_after.pdf}}
\caption{Illustrations for the pre-processing on the depth of the vase.}
\label{fig:pre-processing}
\end{figure}

After the pre-processing procedure, we have an initial smooth and inpainted depth image. 
It will be used as the input of all the depth refinement methods detailed in the following sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{RGBD-Fusion Like method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
RGBD-Fusion is a state-of-the-art depth recovery method proposed by Or-El \emph{et al.}~\cite{or2015rgbd} in 2015.
This novel method is adequate for natural scene illumination and able to enhance the depth map much faster than other methods.
It is reasonable to gain a comprehensive understanding in the field of depth refinement by implementing this method with our own idea inside.

It is worth mentioning that we didn't just follow the paper step by step without injecting any our own ideas.
For example, instead of estimating the pixel-wise ambient light with a separate energy function, we jointly calculated all four first-order spherical harmonics parameters (3 for point-source light direction and 1 for ambient light) with a simple fast least square, and the results have only negligible difference.
And throughout the whole estimation process of light, albedo and depth, we only used the information within the given mask which also speeded up the algorithm.
This is the reason we call our first method "RGB-Fusion Like" method

In the following part of this section, we will explain the method and more our implementation details can be found in Appendix~\ref{appendix:implement}.

%----------------------------------------------
\subsection{Light estimation}
%----------------------------------------------
The natural uncalibrated illumination condition means the light is no longer a point light source, thus a Lambertian model is not sufficient. 
Basri and Jocobs~\cite{basri2003lambertian} has found that low order spherical harmonics (SH) model can well set out the irradiance of the diffused objects under the natural scene.
More specifically, the first-order SH model can capture 87.5\% of natural lighting, whose form is extended from the Lambertian model:
\begin{equation}\label{eq:rgbd_light_model}
I(x,y) = \rho(x,y)(\mathbf{l}^\top \mathbf{n} + \varphi)
\end{equation}
where $I$ is the irradiance of the objects, which is represented as the intensity values, $\rho$ is the albedo, $\mathbf{l}^\top = \begin{pmatrix} l_x & l_y & l_z \end{pmatrix}$ describes the light direction and $\varphi$ represents the ambient light.
Here the surface normal $\mathbf{n}$ is formulated with orthographic projection, i.e. 
\begin{equation}
	\mathbf{n} = \frac{1}{\sqrt{1 + |\nabla z|^2}}
	\begin{pmatrix} 
		 \nabla z\\ 
		 -1
	 \end{pmatrix}
\end{equation}
$\nabla z$ represents the gradient of depth image $z(x,y)$ in $x$ and $y$ directions.
Since we have the input depth from pre-processing, initial $\mathbf{n}_0$ is known.

In the sense of intrinsic image decomposition, an image can be decomposed as the product of albedo and shading, so we can treat $\mathbf{l}^\top \mathbf{n} + \varphi$ in Eq.~\ref{eq:rgbd_light_model} as the shading $S(x,y)$.
Therefore, we have:
\begin{equation}
S(x,y) = \mathbf{l}^\top \mathbf{n} + \varphi = \mathbf{s} \boldsymbol{\cdot} \tilde{\mathbf{n}}
\end{equation}
where
\begin{equation}
\mathbf{s} = \begin{pmatrix}\mathbf{l} \\ \varphi \end{pmatrix}   \; \; \tilde{\mathbf{n}} = \begin{pmatrix}\mathbf{n} \\ 1\end{pmatrix}  
\end{equation}
To compute the spherical harmonics parameters, we assume the albedo $\rho$ equals to $1$ for each pixel. 
Since there are known intensity value and surface normal in each pixel within the mask, we will have an overdetermined least square problem: 
\begin{equation}\label{eq:rgbd_light_estimate}
\min_{\mathbf{s}} \; \lVert I - \mathbf{s} \boldsymbol{\cdot} \tilde{\mathbf{n}} \rVert^2_2
\end{equation}
This process only need to be applied once at the beginning of the process since the least squares is not sensitive to the details on the surface, thus the estimation from the smooth surface is enough.

%----------------------------------------------
\subsection{Albedo estimation}\label{sec:rgbd_albedo_estimation}
%----------------------------------------------
As mentioned in Chapter~\ref{chap:background}, many depth recovery methods based on SFS or photometric stereo techniques assume constant or uniform albedo.
Such assumption does not fit in with the real-world objects, and hence, they perform poorly on the shape estimation for multi-albedo cases.
In order to acquire a satisfying shape outcome, an effective multi-albedo estimation process is a matter of importance.

We know from Eq.~\ref{eq:rgbd_light_model} that, assuming we have the knowledge of input intensity and estimated shading, the albedo image can be directly obtained from $I/S$.
However, such albedo is prone to the overfitting, which make the acquired albedo contain all the undesired spatial layout details.
This is due to the fact that both input image $I$ and the surface normal $\mathbf{n}$ are noisy.
To resolve the overfitting problem, we should impose some restrictions on the estimation of albedo.
A large amount of our daily objects have piecewise smooth appearance, which means most pieces of a layout are dominated by certain colors.
Therefore, a prior that emphasizes the piecewise smoothness on the albedo should be defined.

The albedo of an object can be roughly divided to several pieces with different colors, which we treat it as the image segmentation problem to some extend. 
Thus, we should refer to some classic variational segmentation methods and adapt the edge preserving smoothness term to our problem.
Similar to the idea in~\cite{casaca2014laplacian}, an anisotropic Laplacian term is imposed to estimate the albedo.
The overall regularized minimization problem for albedo estimation is:

\begin{equation}\label{eq:rgbd_albedo_estimate}
	\min_{\rho} \; \lVert I - \rho S\rVert^2_2 + \lambda_{\rho} \lVert \sum_{k \in \mathcal{N}} \omega_k (\rho - \rho_k) \rVert^2_2
\end{equation}
where $k$ indicates the neighbouring index of a certain pixel, which 4-connectivity is chosen in our case. 
The weight $\omega_k$ is defined as below, and it is dependent to two parameters $\sigma_I$ and $\sigma_z$ which accounts for the discontinuity in both intensity and depth.
\begin{equation}
	\omega_k=\exp\Bigg(-\dfrac{\lVert I - I_k \rVert^2_2}{2\sigma_I^2} -\dfrac{\lVert z - z_k \rVert^2_2}{2\sigma_z^2}\Bigg)
\end{equation}

%----------------------------------------------
\subsection{Depth enhancement}
%----------------------------------------------
After acquiring the first-order spherical lighting parameters $\mathbf{s}$ and the albedo $\rho$, we can refine our depth with the help of Eq.~\ref{eq:rgbd_light_model}.
Now our minimization problem with respect to the depth $z$ can be written as below. 
Except for the SFS term, we add a data fidelity term to enable our refined surface close to the input and a smoothness term to make sure that there is no strong discontinuity in the output. 
\begin{equation}\label{eq:rgbd_depth_refine}
	\min_{z} \; \lVert I - \rho (\mathbf{l}^\top \frac{1}{\sqrt{1 + |\nabla z|^2}} \begin{pmatrix} \nabla z\\ -1 \end{pmatrix} + \varphi) \rVert^2_2 + \lambda_z \lVert z - z_0\rVert^2_2 + \lambda_l \lVert \Delta z \rVert^2_2
\end{equation}
where $z_0$ is the input depth and $\Delta$ represents the Laplacian operator. 
It can be easily noticed that this introduced function is non-linear because the normal in our SFS term contains a denominator related to the depth gradient. 
Many optimization methods can be applied to solve the non-linear problem, e.g. Levenberg-Marquardt algorithm or ADMM, but they are not suitable in our application due to expensive computational time. 
Here a "fixed point" method which is similar to iteratively reweighted least square (IRLS) has been introduced to deal with our problem efficiently. 

The idea of the fixed-point approach is in each iteration, the normalizer in the surface normal can be treated as a weighting term and determined by the depth from last iteration.
With the help of this trick, the normalizer is known and Eq.~\ref{eq:rgbd_depth_refine} is linear again.
We can solve the linear system using any fast linear optimization method.
In each iteration $t$, this process can be represented like:
\begin{equation}
	\begin{split}
		\mathbf{n}^{(t)} &= w^{(t)}
		\begin{pmatrix} 
			 \nabla z^{(t)}\\ 
			 -1
	         \end{pmatrix}\\
	         w^{(t)} &=  \frac{1}{\sqrt{1 + |\nabla z^{(t-1)}|^2}}
	\end{split}
\end{equation}
And now the depth refinement problem in Eq.~\ref{eq:rgbd_depth_refine} is reformulated as below in each iteration:

\begin{equation}\label{eq:rgbd_depth_refine2}
	\min_{z} \; \lVert I - \rho (\mathbf{l}^\top \frac{1}{\sqrt{1 + |\nabla z^{(t-1)}|^2}} \begin{pmatrix} \nabla z\\ -1 \end{pmatrix} + \varphi) \rVert^2_2 + \lambda_z \lVert z - z_0\rVert^2_2 + \lambda_l \lVert \Delta z \rVert^2_2
\end{equation}
As long as the energy decreases in each iteration, the process is repeated.

To sum up the approach in this section, it should be noted that the SFS term was used as a core in all light, albedo and depth estimation.
Therefore, we can write an overall energy function for this RGBD-Fusion like method:
\begin{equation}
	E(\rho, z, \mathbf{s}) = \; \lVert I - \rho \mathbf{s}^\top\tilde{\mathbf{n}}(z) \rVert^2_2 + \lambda_{\rho} \lVert \sum_{k \in \mathcal{N}} \omega_k (\rho - \rho_k) \rVert^2_2 + \lambda_z \lVert z - z_0\rVert^2_2 + \lambda_l \lVert \Delta z \rVert^2_2
\end{equation}

Finally, the whole process of RGBD-Like method has been described in Alg.~\ref{alg:rgbd_fusion}. 



\begin{algorithm}[!htbp]
	\begin{algorithmic}[1]
  		\caption{\textbf{RGBD-Fusion Like Depth Refinement}}
		\label{alg:rgbd_fusion}
		 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
		 \renewcommand{\algorithmicensure}{\textbf{Output:}}
		 \REQUIRE Initial depth image $z_0$, RGB image $I$
		 \vspace{1.8mm}
		 \STATE Estimate SH parameter, $\mathbf{s} = \argmin \limits_{\mathbf{s}} \; E(\rho = 1, z_0)$ \COMMENT{Eq.~\ref{eq:rgbd_light_estimate}}
		 \STATE Estimate albedo, $\rho = \argmin \limits_{\rho} \; E(z_0, \mathbf{s})$ \COMMENT{Eq.~\ref{eq:rgbd_albedo_estimate}}
		 \STATE t = 1, $z^{(t-1)} = z_0$
		 \vspace{1.8mm}
		  \WHILE {$ E(\rho, z^{(t)}, \mathbf{s}) - E(\rho, z^{(t-1)}, \mathbf{s}) < 0$}
		   \vspace{1.8mm}
			  \STATE $z^{(t)} = \argmin \limits_{z} E(\rho, z, \mathbf{s})$ \COMMENT{Eq.~\ref{eq:rgbd_depth_refine}}
		          \STATE $t := t + 1$
		 \vspace{1.8mm}
		  \ENDWHILE
		  \ENSURE  Refined depth image $z^{(t)}$
	\end{algorithmic}
\end{algorithm}


%----------------------------------------------
\subsection{Limitations}
%----------------------------------------------

Although our RGBD-Fusion like method works moderately well in many real cases, it is not difficult to find the limitations and improve correspondingly.
\begin{itemize}
\item the surface normal modelled by the orthographic projection is merely an ideal case, but it is not really in line with the real world camera model.
And the intrinsic parameters such as the focal length and the coordinate of the principle point are either usually given as a preliminary knowledge, or obtained from calibration without much effort.
Hence, it is reasonable to formulate the surface normal with the perspective projection model.

\item In our RGBD-Fusion like method, only the intensity is applied because the values in RGB channels are more or less the same under the natural scene illumination. 
When we estimated the SH lighting parameters and the albedo in 3 channels separately, the results are quite similar to each other.
So using all three channel rather than just the intensity value will not provide much extra information and improve the depth enhancement. 
Instead, it will just decelerate the whole algorithm.
We ought to find a way to take better advantages of all three channels.

\item The most important inspiration for us to propose RGB ratio model in the next section is, the RGBD-Fusion like method was not convergent in terms of depth enhancement part because of the fix-point method.
In the $4$th line of the Alg.~\ref{alg:rgbd_fusion}, we make the iteration stop when the energy for the depth refinement starts increasing.
This is due to the reason that the fixed-point method actually tries to solve the non-linearity in a tricky way, which is mathematically not totally correct. 
Therefore, we thought of the idea of RGB ratio model, which can eliminate the denominator inside the normal and promise a real linear problem.

\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed method I: RGB Ratio Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

According to the limitations in the last section, we thought of the idea of RGB ratio model.
First of all, we replace the orthographic projection model with the perspective one.
And then, to fully use the information of the RGB three channels while eliminating the non-linearity in the objective function in the depth refinement, we use the ratio model between every two channels among the three.

It should be noted that we need to add active R, G and B 3 LED lights for the sake of emphasizing the difference among RGB channels.
The green LED is installed in the middle with the red and blue ones on the two sides of ASUS Xtion Pro Live camera (both are around 30 cm to the green LED).
The hardware setup and a color image taken with such setup are illustrated in Fig.~\ref{fig:ratio_setup}.

\begin{figure}[!htbp]
\centering
\subfigure[LED lights setup]{\includegraphics[width=0.45\linewidth]{figures/methodology/ratio_setup.JPG}}
\subfigure[A color image with our lighting setup]{\includegraphics[width=0.45\linewidth]{figures/methodology/ratio_setup_view.png}}
\caption{Illustrations for the RGB LED setup and the corresponding image.}
\label{fig:ratio_setup}
\end{figure}


Now to derive our new ratio model, we treat each channel of the color image $I$ as an single intensity image, denoted by $I_R, I_G, I_B$.
Therefore, 3 equations can be obtained from Eq.~\ref{eq:rgbd_light_model}.

\begin{equation}\label{eq:ratio_prepare}
    \begin{split}
	I_R &= \rho_R(\mathbf{l}_R^\top \mathbf{n} + \varphi_R)\\
	I_G &= \rho_G(\mathbf{l}_G^\top \mathbf{n} + \varphi_G)\\
	I_B &= \rho_B(\mathbf{l}_B^\top \mathbf{n} + \varphi_B)
    \end{split}
\end{equation}

Using R and G channel as an example, we acquire the the ratio model:

\begin{equation}\label{eq:ratio_rg_sh1}
%\begin{split}
\frac{I_R - \rho_R \varphi_R}{I_G - \rho_G \varphi_G} = \frac{\rho_R \mathbf{l}_R^\top \mathbf{n}}{\rho_G \mathbf{l}_G^\top \mathbf{n}}
%\rho_G (I_R - \rho_R \varphi_R)\mathbf{l}_G^T \mathbf{n} & = \rho_R (I_G - \rho_G \varphi_G)\mathbf{l}_R^T\mathbf{n}\\
%\end{split}
\end{equation}

Similarly, we can acquire another two ratio models which are between green and blue, and blue and red channels respectively.
We are able to notice from Eq~\ref{eq:ratio_rg_sh1} that, the non-linearity problem mentioned before has been solved because the denominator in the surface normal $\mathbf{n}$ is cancelled out.
Also, our normal is derived from perspective camera model and can be represented as a function of $\; \log z$. 
For the sake of simplicity we directly represent $z = \log z$ and redefine $\mathbf{n}$ without the normalizer in the following part.
%$$$$$$$$$$$$$$$$
\begin{equation}\label{eq:ratio_normal}
    \mathbf{n} =
     \begin{pmatrix}
         fz_x\\
         fz_y\\
         -1 - \tilde{x}z_x - \tilde{y}z_y
     \end{pmatrix}
\end{equation}
%$$$$$$$$$$$$$$$$
where $f$ is the focal length, $(\tilde{x}, \tilde{y}) = (x- x_0, y - y_0)$, with $(x_0, y_0)$ the coordinates of principle points and $(x,y)$ the coordinate of a pixel inside the given mask, $[z_x, z_y]$ is the gradient of depth $z$.

Now we will explain our proposed algorithm based on the new ratio model.

%The key to this ratio model is toenlarge the difference of pixel values, light directions and the albedo in 3 channels

%----------------------------------------------
\subsection{Algorithm details}
%----------------------------------------------
Similar to the RGBD-Fusion Like method, the algorithm is separated to 3 parts: light estimation, albedo estimation and depth enhancement.
However, our new method requires an initial estimation of the color albedo as the input of our iterative method, and hence, we calculate the initial SH parameters $\mathbf{l}^{0}$ with Eq.~\ref{eq:rgbd_light_model} and then the albedo $\rho^{0}$ with Eq.~\ref{eq:rgbd_albedo_estimate} using the old model.
Noted that the initial estimation is performed with respect to all RGB three channels.

\textbf{Albedo refinement:}
with the acquired $\rho^{0}$ and $\mathbf{l}^{0}$, we can start the iteratively refinement process.
In order to refine the color albedo with our ratio model, in each iteration, we need to reshape the ratio model described in Eq.~\ref{eq:ratio_rg_sh1} as follows:
\begin{equation}\label{eq:ratio_albedo_prepare}
\begin{split}
I_G \mathbf{l}_R^\top\mathbf{n} \rho_R - I_R \mathbf{l}_G^\top\mathbf{n} \rho_G  = \rho_R \rho_G (\varphi_G\mathbf{l}_R^\top \mathbf{n} - \varphi_R\mathbf{l}_G^\top \mathbf{n})\\
I_B \mathbf{l}_G^\top\mathbf{n} \rho_G - I_G \mathbf{l}_B^\top\mathbf{n} \rho_B  = \rho_G \rho_B (\varphi_B\mathbf{l}_G^\top \mathbf{n} - \varphi_G\mathbf{l}_B^\top \mathbf{n})\\
I_R \mathbf{l}_B^\top\mathbf{n} \rho_B - I_B \mathbf{l}_R^\top\mathbf{n} \rho_R  = \rho_B \rho_R (\varphi_R\mathbf{l}_B^\top \mathbf{n} - \varphi_B\mathbf{l}_R^\top \mathbf{n})
\end{split}
\end{equation}
For each pixel, we can reformulate the Eq.~\ref{eq:ratio_albedo_prepare} to a matrix form:
%$$$$$$$$$$$$$$$$
\begin{equation}\label{eq:rho_matrix}
    \begin{pmatrix}
        I_G \mathbf{l}_R^\top\mathbf{n} & - I_R \mathbf{l}_G^\top\mathbf{n} & 0 \\
        0 & I_B \mathbf{l}_G^\top\mathbf{n} & - I_G \mathbf{l}_B^\top\mathbf{n} \\
        - I_B \mathbf{l}_R^\top\mathbf{n} & 0 & I_R \mathbf{l}_B^\top\mathbf{n} 
    \end{pmatrix}
    \begin{pmatrix}
        \rho_R\\
        \rho_G\\
        \rho_B
     \end{pmatrix}_{3 \times 1}
    =
    \begin{pmatrix}
        \rho_R \rho_G (\varphi_G\mathbf{l}_R^\top \mathbf{n} - \varphi_R\mathbf{l}_G^\top \mathbf{n})\\
        \rho_G \rho_B (\varphi_B\mathbf{l}_G^\top \mathbf{n} - \varphi_G\mathbf{l}_B^\top \mathbf{n})\\
        \rho_B \rho_R (\varphi_R\mathbf{l}_B^\top \mathbf{n} - \varphi_B\mathbf{l}_R^\top \mathbf{n})
    \end{pmatrix}
\end{equation}
which can be denoted as $\mathbf{A}_{\rho}\cdot \rho = \mathbf{b}_{\rho}$ and for the sake of simplicity, $\rho$ in this section represents the stack of RGB three albedos.
%$$$$$$$$$$$$$$$$

To acquire the RGB albedos, some regularization terms are required similar to Eq.~\ref{eq:rgbd_albedo_estimate}. The minimization problem of color albedo in each iteration now becomes:
%$$$$$$$$$$$$$$$$
\begin{equation}\label{eq:ratio_albedo_refine}
\rho^{(t)} = \argmin_{\rho} \lVert\mathbf{A}_{\rho}^{(t-1)}\rho - \mathbf{b}_{\rho}^{(t-1)}\rVert^2 + \lambda_{\rho}^1 \lVert \omega \nabla \rho \rVert^2  + \lambda_{\rho}^2 \lVert \rho - \rho^{(t-1)}\rVert^2
\end{equation}
where the weight $\omega = [\omega_R, \omega_G, \omega_B]$, which can be denoted as:
\begin{equation}
\omega_i = \exp(- \frac{\sigma_i ||\nabla I_i||^2}{\max ||\nabla I_i||^2}), \quad i \in \{R,G,B\}
\end{equation}
$\sigma$ is a tuning parameter for each channel.  

%$$$$$$$$$$$$$$$$
There are three interesting aspects about the albedo estimation which worth having a few more words:
\begin{enumerate}
    \item One observation about Eq.~\ref{eq:rho_matrix} is that, if the SH parameters are the same among the three channels, the right side of the equal sign is or close to 0. 
    This is the reason why we need to set up 3 LED lights with a distance to each other, which will provide us enough difference on the light directions.
    \item Instead of using anisotropic Laplacian regularization in RGBD-Like method, the smoothness term in Eq.~\ref{eq:ratio_albedo_refine} only takes the use of the gradient of $\rho$ with a weight only depending on the RGB image's gradient.
    It takes less efforts to build such a smoothness term than the anisotropic term, but the acquired albedo is still satisfying.
    \item If we don't use a data fidelity term $\lVert \rho - \rho^{(t-1)}\rVert^2$, the albedo will get increasingly dark after several iterations. 
    This is due to the fact that there also exist the RGB albedos in $\mathbf{b}_{\rho}$, so $\rho = 0$ will become the solution of our ratio model term.
    Therefore, adding the data term can not only avoid such problem, but help refine the albedo iteratively. 
\end{enumerate}

{\color{red} need to add the albedo estimation 1, ground truth, 2, without regularization, 3, without weighting, 4 with weighting}

\textbf{Depth refinement}:
After acquiring the color albedo in time step $t$, we are going to refine the depth with the help of the ratio model.
First we reshape Eq.~\ref{eq:ratio_rg_sh1} with the surface normal $\mathbf{n}$ as the argument:
%$$$$$$$$$$$$$$$$
\begin{equation}\label{eq:ratio_depth1}
\begin{split}
\rho_G (I_R - \rho_R \varphi_R)\mathbf{l}_G^T \mathbf{n} - \rho_R (I_G - \rho_G \varphi_G)\mathbf{l}_R^T\mathbf{n} = 0\\
\rho_B (I_G - \rho_G \varphi_G)\mathbf{l}_B^T \mathbf{n} - \rho_G (I_B - \rho_B \varphi_B)\mathbf{l}_G^T\mathbf{n} = 0\\
\rho_R (I_B - \rho_B \varphi_B)\mathbf{l}_R^T \mathbf{n} - \rho_B (I_R - \rho_R \varphi_R)\mathbf{l}_B^T\mathbf{n} = 0 
\end{split}
\end{equation}
since the normal $\mathbf{n}$ now is a function of $z$, Eq.~\ref{eq:ratio_depth1} can be actually simplified as below (the derivation details can be found in Appendix~\ref{appendix:implement}):
\begin{equation}
    \Psi z = 0
\end{equation}

Correspondingly, the minimization for perspective model becomes:

\begin{equation}\label{eq:ratio_depth_refine}
z^{(t)}= \argmin_{z}  ||\Psi z||^2 + \lambda_z||z - z_0||^2
\end{equation}

The overall energy for the proposed RGB ratio model method is:
\begin{equation}
	E(\rho^{(t)}, z^{(t)}) = \; \lVert Ratio(\rho^{(t)}, z^{(t)}) \rVert^2_2 + \rVert^2 + \lambda_z||z^{(t)} - z_0||^2 + \lambda_{\rho}^1 \lVert \omega \nabla \rho^{(t)} \rVert^2  + \lambda_{\rho}^2 \lVert \rho - \rho^{(t-1)}\rVert^2
\end{equation}


\begin{algorithm}[!htbp]
	\begin{algorithmic}[1]
  		\caption{\textbf{RGB Ratio Model method}}
		\label{alg:rgb_ratio}
		 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
		 \renewcommand{\algorithmicensure}{\textbf{Output:}}
		 \REQUIRE Initial depth image $z_0$, RGB image $I$, mask, focal length, principle point
		 \vspace{1.8mm}
		 \STATE Estimate initial SH parameter $\mathbf{s}^{(0)}$ for each cahnnel \COMMENT{Eq.~\ref{eq:rgbd_light_estimate}}
		 \STATE Estimate initial color albedo $\rho^{(0)}$  \COMMENT{Eq.~\ref{eq:rgbd_albedo_estimate}}
		 \STATE t = 1, $z^{(t-1)} = z_0$
		 \vspace{1.8mm}
		  \WHILE {$ \frac{\Vert E(\rho^{(t)}, z^{(t)}) - E(\rho^{(t-1)}, z^{(t-1)})\Vert}{E(\rho^{(t-1)}, z^{(t-1)})} > \epsilon$}
		   \vspace{1.8mm}
			\STATE $\rho^{(t)} = \argmin \limits_{\rho} E(\rho^{(t-1)}, z^{(t-1)})$ \COMMENT{Eq.~\ref{eq:ratio_albedo_refine}}
			  \STATE $z^{(t)} = \argmin \limits_{z} E(\rho^{(t)})$ \COMMENT{Eq.~\ref{eq:ratio_depth_refine}}
			  \STATE Update $\mathbf{s}^{(t)}$ \COMMENT{Eq.~\ref{eq:rgbd_light_estimate}}
			  \vspace{1.8mm}
		          \STATE $t := t + 1$
		 \vspace{1.8mm}
		  \ENDWHILE
		  \ENSURE  Refined depth image $z^{(t)}$ and albedo $\rho^{(t)}$
	\end{algorithmic}
\end{algorithm}

%----------------------------------------------
\subsection{Limitations}
%----------------------------------------------
Our method can estimate the albedo and the depth better than our RGBD-Fusion like method in some cases because the non-linearity optimization problem for the RGBD-Fusion like method has been solved.
Still, there exist some defects for our new RGB ratio model.
\begin{itemize}
\item Three LED lights have to be set up far away from each other.
	As already mentioned about Eq.~\ref{eq:ratio_albedo_prepare} , the albedo refinement may fail if the lights are too close and thus, we need to put 3 lights too near. 
	This can lead to some inconvenience, such as the requirement of enough space to put the system.
\item RGB three lights is likely to bring extra specularity.
	 If we want to refine the depth of an specular objects, the specular reflection will be from not only the natural scene illumination, but also RGB lights from 3 directions, which will make the refined results even worse.
	 
\item Auto white balance (AWB) has a big impact on the refined results.
	This is due to the fact that the success of our model highly relies on the difference among 3 channels in a color image.
	And AWB will mix up the information in 3 channels so it is very necessary to turn it off.
	This impedes the generalization of our model because AWB has been set as a default in many modern inexpensive cameras.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed method II: Robust Lighting Variation Model}

%----------------------------------------------
\subsection{Inspiration}
%----------------------------------------------
We can notice that the albedo estimation of both our RGBD-Like method and RGB ratio model is highly dependent on the regularization terms which emphasize the piece-wise smoothness.
This is a standard approach for almost all the state-of-the-art depth refinement method to estimate the albedo.
They work fine when the albedo itself is very simple with big patches of patterns and only several dominant colors.
However, there are more real-world objects containing complicated layout colors and patterns which all these methods with such a process of albedo estimation will fail.
Were the albedo estimation not working, the outcome of final depth refinement has no chance to be corrrect. 
What is more, The parameters for the regularization terms are often needed to be different for various object, so the parameters tuning for the regularization is tedious and time-consuming.

As a consequence, it is reasonable to propose a new method which is able to eliminate all the regularization terms and estimate complicated albedo.
The necessity of using regularizations for calculating albedo have been mentioned in section~\ref{sec:rgbd_albedo_estimation}, which in short is about avoiding the overfitting problem if only the shading term is applied.
Provided we have several color images for a still object with light coming from various directions, the shading term in Eq.~\ref{eq:rgbd_albedo_estimate} without the need of regularizations is sufficient for estimating the albedo.
{\color{blue} (not sure) Assuming the $n$ lights directions are estimated while the rough surface normal and $n$ color images are given, in this case, computing the albedo with a least square can resolve the overfitting problem.}

In order to simulate the scenario that a direct light comes from different directions, we simply sway a white LED light in different directions and take several images (even just the flash lamp on any phone is okay). An example of a vase from different lighting directions are shown in Fig.~\ref{fig:robust_setup}.

\begin{figure}[!htbp]
\centering
\subfigure{\includegraphics[width=0.23\linewidth]{figures/methodology/robust_setup1.png}}
\subfigure{\includegraphics[width=0.23\linewidth]{figures/methodology/robust_setup2.png}}
\subfigure{\includegraphics[width=0.23\linewidth]{figures/methodology/robust_setup3.png}}
\subfigure{\includegraphics[width=0.23\linewidth]{figures/methodology/robust_setup4.png}}
\caption{Illustrations for the obtained color images of a vase from various light directions with a white LED light.}
\label{fig:robust_setup}
\end{figure}
 
{\color{red} put a real-world albedo estimate case that RGBD-fusion fails but our method works}


%----------------------------------------------
\subsection{Algorithm details}
%----------------------------------------------
Since we don't control with RGB LED lights anymore but one white light, the ratio model in Eq.~\ref{eq:ratio_rg_sh1} is not applicable anymore, so we need to use the standard 1st-order SH model described in Eq.~\ref{eq:rgbd_light_model} again to construct the input color images.

Again, the proposed algorithm consists of three part: light estimation, albedo estimation and depth enhancement. 
We need to iteratively update the light directions, color albedo and the depth, but unlike the RGB ratio model method, we don't need to have an initial estimated light and albedo beforehand.
Instead, we can just assume the albedo to be 1 everywhere at the beginning and then start refining everything reiteratively in the loop, as shown in Alg.~\ref{alg:robust}.

\textbf{Light estimation}
Similar to Eq.~\ref{eq:rgbd_light_estimate}, we refine the SH parameters for each input image $i$ in each iteration using the formulation below:

\begin{equation}\label{eq:robust_light_estimate}
\min_{\mathbf{s}_i} \; \lVert I_i - \rho \boldsymbol{\cdot} \mathbf{s}_i^\top \tilde{\mathbf{n}} \rVert^2_2 \; \; i \in \{1, \cdots, n\}
\end{equation}
where
\begin{equation}
\mathbf{s}_i = \begin{pmatrix}\mathbf{l}_i \\ \varphi_i \end{pmatrix}   \; \; \tilde{\mathbf{n}} = \begin{pmatrix}\mathbf{n} \\ 1\end{pmatrix}  
\end{equation}

\textbf{Albedo estimation}
\begin{equation}\label{eq:robust_albedo_estimate}
	\min_{\rho} \; \lVert \mathbb{I} - \rho \boldsymbol{\cdot} \mathbb{S} \tilde{\mathbf{n}} \rVert^2_2 
\end{equation}
where
\begin{equation}
\mathbb{I} = \begin{pmatrix} I_1 \\ \vdots \\ I_n \end{pmatrix}   \; \; \mathbb{S} = \begin{pmatrix} \mathbf{s}_1^\top \\ \vdots \\ \mathbf{s}_n^\top \end{pmatrix}  
\end{equation}


\begin{algorithm}[!htbp]
	\begin{algorithmic}[1] 
  		\caption{\textbf{Robust Light Variation Model Method}}
		\label{alg:robust}
		 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
		 \renewcommand{\algorithmicensure}{\textbf{Output:}}
		 \REQUIRE Initial depth image $z_0$, RGB image $I$, mask, focal length, principle point
		 \vspace{1.8mm}
		 \STATE Estimate initial SH parameter $\mathbf{s}^{(0)}$ for each cahnnel \COMMENT{Eq.~\ref{eq:rgbd_light_estimate}}
		 \STATE Estimate initial color albedo $\rho^{(0)}$  \COMMENT{Eq.~\ref{eq:rgbd_albedo_estimate}}
		 \STATE t = 1, $z^{(t-1)} = z_0$
		 \vspace{1.8mm}
		  \WHILE {$ \frac{\Vert E(\rho^{(t)}, z^{(t)}) - E(\rho^{(t-1)}, z^{(t-1)})\Vert}{E(\rho^{(t-1)}, z^{(t-1)})} > \epsilon$}
		   \vspace{1.8mm}
			\STATE $\rho^{(t)} = \argmin \limits_{\rho} E(\rho^{(t-1)}, z^{(t-1)})$ \COMMENT{Eq.~\ref{eq:ratio_albedo_refine}}
			  \STATE $z^{(t)} = \argmin \limits_{z} E(\rho^{(t)})$ \COMMENT{Eq.~\ref{eq:ratio_depth_refine}}
			  \STATE Update $\mathbf{s}^{(t)}$ \COMMENT{Eq.~\ref{eq:rgbd_light_estimate}}
			  \vspace{1.8mm}
		          \STATE $t := t + 1$
		 \vspace{1.8mm}
		  \ENDWHILE
		  \ENSURE  Refined depth image $z^{(t)}$ and albedo $\rho^{(t)}$
	\end{algorithmic}
\end{algorithm}

%----------------------------------------------
\subsection{Deal with specular objects}
%----------------------------------------------


%----------------------------------------------
\subsection{Depth super-resolution}
%----------------------------------------------


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

